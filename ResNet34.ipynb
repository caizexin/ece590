{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConvLayer(nn.Module):\n",
    "    def __init__(self, input_filters, output_filters, kernel_size=(3,3), strides=(1,1), padding_size=1):\n",
    "        super(BasicConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_filters, output_filters, kernel_size, padding=padding_size)\n",
    "        self.bn = nn.BatchNorm2d(output_filters)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, conv_path=False, input_filters=64, output_filters=64):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_path = conv_path\n",
    "        if conv_path:\n",
    "            assert input_filters != output_filters\n",
    "            self.conv = BasicConvLayer(input_filters, output_filters, (1,1), padding_size=0)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        if self.conv_path:\n",
    "            conv_x = self.conv(x)\n",
    "            return F.relu(conv_x + sublayer(x))\n",
    "        return F.relu(x + sublayer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_filters, output_filters, kernel_size=(3,3), strides=(1,1)):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = BasicConvLayer(input_filters, output_filters, kernel_size, strides)\n",
    "        self.conv2 = BasicConvLayer(output_filters, output_filters, kernel_size, strides)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv2(F.relu(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.convBN = BasicConvLayer(input_size, 32, kernel_size=(2,2), strides=(1,1))\n",
    "        self.maxP = nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "        self.resLayers = clones(ResidualBlock(), 13)\n",
    "        self.resconvLayer1 = ResidualBlock(conv_path=True, input_filters=32, output_filters=64)\n",
    "        self.resconvLayer2 = ResidualBlock(conv_path=True, input_filters=64, output_filters=128)\n",
    "        self.resconvLayer3 = ResidualBlock(conv_path=True, input_filters=128, output_filters=256)\n",
    "        self.transconvBn1 = ConvBlock(32, 64)\n",
    "        self.transconvBn2 = ConvBlock(64, 128)\n",
    "        self.transconvBn3 = ConvBlock(128, 256)\n",
    "        self.layer1 = clones(ConvBlock(32, 32), 3)\n",
    "        self.layer2 = clones(ConvBlock(64, 64), 3)\n",
    "        self.layer3 = clones(ConvBlock(128, 128), 5)\n",
    "        self.layer4 = clones(ConvBlock(256, 256), 2)\n",
    "        self.avgP = nn.AvgPool2d((3,3), stride=(2,2))\n",
    "        self.linear1 = nn.Linear(12544, 4096)\n",
    "        self.linear2 = nn.Linear(4096, 1024)\n",
    "        self.linear3 = nn.Linear(1024, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxP(self.convBN(x))\n",
    "        x = self.resLayers[0](x, self.layer1[0])\n",
    "        x = self.resLayers[1](x, self.layer1[1])\n",
    "        x = self.resLayers[2](x, self.layer1[2])\n",
    "        x = self.resconvLayer1(x, self.transconvBn1)\n",
    "        x = self.resLayers[3](x, self.layer2[0])\n",
    "        x = self.resLayers[4](x, self.layer2[1])\n",
    "        x = self.resLayers[5](x, self.layer2[2])\n",
    "        x = self.resconvLayer2(x, self.transconvBn2)\n",
    "        x = self.resLayers[6](x, self.layer3[0])\n",
    "        x = self.resLayers[7](x, self.layer3[1])\n",
    "        x = self.resLayers[8](x, self.layer3[2])\n",
    "        x = self.resLayers[9](x, self.layer3[3])\n",
    "        x = self.resLayers[10](x, self.layer3[4])\n",
    "        x = self.resconvLayer3(x, self.transconvBn3)\n",
    "        x = self.resLayers[11](x, self.layer4[0])\n",
    "        x = self.resLayers[12](x, self.layer4[1])\n",
    "        x = self.avgP(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return F.softmax(self.linear3(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 100\n",
    "INITIAL_LR = 0.05\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-4\n",
    "EPOCHS = 700\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "'''\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])])\n",
    "'''\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),])\n",
    "\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = ResNet(3, 10)\n",
    "\n",
    "for p in net.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_normal(p)\n",
    "\n",
    "net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = True\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model/resnet.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "#criterion = lambda y_pred, y : -y_pred[y] + torch.log(torch.sum(torch.exp(y_pred)))\n",
    "# Add optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # Train the training dataset for 1 epoch.\n",
    "    #print(len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += inputs.shape[0]\n",
    "        correct_examples +=  (predicted == targets).sum()\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = float(correct_examples) / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    #print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Calculate accuracy\n",
    "            total_examples += inputs.shape[0]\n",
    "            correct_examples += (predicted == targets).sum()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = float(correct_examples) / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "\n",
    "    DECAY_EPOCHS = 10\n",
    "    DECAY = 0.9\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate = current_learning_rate * DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        #print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'resnet2.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "all_predicted = [['Id', 'Category']]\n",
    "\n",
    "test_set = np.load('cifar10-batches-images-test.npy')\n",
    "#test_set = test_set.reshape((10000, 3, 32, 32))\n",
    "#print(test_set[1][0])\n",
    "tmp_test_set = []\n",
    "for x in test_set:\n",
    "    tmp_test_set.append(transform_val(x).view((3,32,32)).data.numpy())\n",
    "print(tmp_test_set[3].shape)\n",
    "test_set = np.array(tmp_test_set)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        tmp_set = test_set[i*1000:(i+1)*1000,:,:,:]\n",
    "        inputs = torch.tensor(tmp_set,dtype=torch.float32)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "            \n",
    "        predicted = [x for x in enumerate(predicted.data.cpu().numpy())]\n",
    "        print(len(predicted))\n",
    "        for x in predicted:\n",
    "            tmp = list(x)\n",
    "            tmp[0] += i*1000\n",
    "            all_predicted.append(tmp)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('result.csv', 'w')\n",
    "for i in all_predicted:\n",
    "    f.write(str(i[0])+','+str(i[1]) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ece590",
   "language": "python",
   "name": "ece590"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
